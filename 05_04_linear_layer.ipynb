{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YonggunJung/Fastcompus/blob/main/05_04_linear_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MiNDP8Xnel1"
      },
      "source": [
        "# Linear Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xIFGjWTnel5"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB0W4rUNnel7"
      },
      "source": [
        "## Raw Linear Layer\n",
        "\n",
        "$$\\begin{gathered}\n",
        "y=x\\cdot{W}+b, \\\\\n",
        "\\text{where }x\\in\\mathbb{R}^{N\\times{n}}\\text{, }y\\in\\mathbb{R}^{N\\times{m}}. \\\\\n",
        "\\\\\n",
        "\\text{Thus, }W\\in\\mathbb{R}^{n\\times{m}}\\text{ and }b\\in\\mathbb{R}^m.\n",
        "\\end{gathered}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJEowSFMnel7"
      },
      "outputs": [],
      "source": [
        "W = torch.FloatTensor([[1, 2],\n",
        "                       [3, 4],\n",
        "                       [5, 6]])\n",
        "b = torch.FloatTensor([2, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Thh-6J7nel8",
        "outputId": "631cd244-31af-41d0-d620-f987d6e95cbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2])\n",
            "torch.Size([2])\n"
          ]
        }
      ],
      "source": [
        "print(W.size())\n",
        "print(b.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tBsrs3ynel9"
      },
      "outputs": [],
      "source": [
        "def linear(x, W, b):\n",
        "    y = torch.matmul(x, W) + b\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5bBdfrWnel9",
        "outputId": "67857e7f-e0fe-4d00-9ee9-5938a3a6620c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3])\n"
          ]
        }
      ],
      "source": [
        "x = torch.FloatTensor([[1, 1, 1],\n",
        "                       [2, 2, 2],\n",
        "                       [3, 3, 3],\n",
        "                       [4, 4, 4]])\n",
        "\n",
        "print(x.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Icrp-82nel-"
      },
      "outputs": [],
      "source": [
        "y = linear(x, W, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uumNSS44nel-",
        "outputId": "8b56dea9-4ec2-40dc-d4ef-099d6d20c78a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2])\n"
          ]
        }
      ],
      "source": [
        "print(y.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a797pj4Mnel_"
      },
      "source": [
        "## nn.Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFNNrb1Rnel_"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsM8QjCwnel_"
      },
      "outputs": [],
      "source": [
        "class MyLinear(nn.Module):  # nn.Module을 상속 받는다\n",
        "# 그리고 보통 __init__함수와 forward함수를 오버라이드한다\n",
        "\n",
        "    # 나중에 forward에서 쓸 것들을 미리 설정\n",
        "    def __init__(self, input_dim=3, output_dim=2):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.W = torch.FloatTensor(input_dim, output_dim)\n",
        "        self.b = torch.FloatTensor(output_dim)\n",
        "\n",
        "    # You should override 'forward' method to implement detail.\n",
        "    # The input arguments and outputs can be designed as you wish.\n",
        "    def forward(self, x):\n",
        "        # |x| = (batch_size, input_dim)\n",
        "        y = torch.matmul(x, self.W) + self.b\n",
        "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
        "        #     = (batch_size, output_dim)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sip6pRKOnemA"
      },
      "outputs": [],
      "source": [
        "linear = MyLinear(3, 2)\n",
        "\n",
        "y = linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzGs--3NnemA",
        "outputId": "58977fda-cd77-451b-d0cc-b8ef72ee6370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2])\n"
          ]
        }
      ],
      "source": [
        "print(y.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b53fwT2VnemA"
      },
      "outputs": [],
      "source": [
        "for p in linear.parameters():\n",
        "    print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aIBpp6NnemA"
      },
      "source": [
        "You can see that there is no weight parameters to learn.\n",
        "Above way can forward(or calculate) values, but it cannot be trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFroEhSSnemB"
      },
      "source": [
        "### Correct way: nn.Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_9onUpXnemB"
      },
      "outputs": [],
      "source": [
        "class MyLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=3, output_dim=2):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
        "        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # |x| = (batch_size, input_dim)\n",
        "        y = torch.matmul(x, self.W) + self.b\n",
        "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
        "        #     = (batch_size, output_dim)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHZfuSZWnemB"
      },
      "source": [
        "Reference: https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter\n",
        "\n",
        "A kind of Tensor that is to be considered a module parameter.\n",
        "\n",
        "Parameters are Tensor subclasses, that have a very special property when used with Module s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLZLWPNOnemB"
      },
      "outputs": [],
      "source": [
        "linear = MyLinear(3, 2)\n",
        "\n",
        "y = linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYp-6G1dnemC",
        "outputId": "27e1289f-30df-4dbf-f4e1-b6825c51a46c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2])\n"
          ]
        }
      ],
      "source": [
        "print(y.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLklNyRjnemC",
        "outputId": "8293439d-81d4-452d-b7fc-92cae0cf0d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0000e+00, -0.0000e+00],\n",
            "        [ 9.1002e+31, -2.8586e-42],\n",
            "        [ 8.4078e-45,  0.0000e+00]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([4.7428e+30, 7.1429e+31], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "for p in linear.parameters():\n",
        "    print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxSg4fbknemC"
      },
      "source": [
        "## nn.Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDkxC2BQnemC"
      },
      "outputs": [],
      "source": [
        "# 이걸 하면 위에껄 할 필요 없음\n",
        "linear = nn.Linear(3, 2)\n",
        "\n",
        "y = linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucL4QbJynemC",
        "outputId": "3de461c0-e9fe-4497-f38c-57c5bfa7e499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2])\n"
          ]
        }
      ],
      "source": [
        "print(y.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "fa88qMhinemD",
        "outputId": "569ddc32-500e-4234-bdb5-791778f8a199"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.3386, -0.5355, -0.4991],\n",
            "        [ 0.1993,  0.4776,  0.1894]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1819, 0.0941], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "for p in linear.parameters():\n",
        "    print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24ROhQhznemD"
      },
      "source": [
        "### nn.Module can contain other nn.Module's child classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mDibTDdnemD"
      },
      "outputs": [],
      "source": [
        "class MyLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=3, output_dim=2):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # |x| = (batch_size, input_dim)\n",
        "        y = self.linear(x)\n",
        "        # |y| = (batch_size, output_dim)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGuw3oVsnemD"
      },
      "outputs": [],
      "source": [
        "linear = MyLinear(3, 2)\n",
        "\n",
        "y = linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRl3RfEanemD",
        "outputId": "31eb768f-f134-49fa-a97e-480dbcc2dadd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2])\n"
          ]
        }
      ],
      "source": [
        "print(y.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxxbF_W_nemD",
        "outputId": "4ea06f1f-35d8-4fc9-c278-ce9f8e07e751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.1267,  0.0563,  0.3951],\n",
            "        [ 0.2291,  0.3214,  0.2595]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3659, 0.4013], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "for p in linear.parameters():\n",
        "    print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fEjzs1pnemE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}